{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs0VQK0Nc-Rj"
   },
   "source": [
    "## Очистка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "RZ5vZFWEctos",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "96706390-f844-4575-b8e2-d5190f7b7da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZpmZlcN-ctcr"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIwVP7ivctaA",
    "outputId": "94389ce1-d1d1-45f4-e513-d9591668b2c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"d0rj/geo-reviews-dataset-2023\")\n",
    "work_data = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POlHzknictXQ"
   },
   "outputs": [],
   "source": [
    "work_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSjqUpZTctUg"
   },
   "outputs": [],
   "source": [
    "work_data = work_data.dropna(subset=['text', 'name_ru', 'rating'])\n",
    "work_data = work_data.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "work_data['text'] = work_data['text'].str.replace('\\\\n', ' ')\n",
    "work_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_hXfg_kctR9"
   },
   "outputs": [],
   "source": [
    "work_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LxJiyZT64sc"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_review_text(text: str) -> str:\n",
    "    text = text.lower()  # Приводим к нижнему регистру\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)  # Удаляем HTML-теги\n",
    "    text = re.sub(r\"[^\\w\\s,.!?()]+\", \"\", text)  # Удаляем спецсимволы, кроме пунктуации и скобок\n",
    "    text = re.sub(r\"\\)\\)+\", \" \", text)  # Заменяем смайлики вида \"))\" на пробел\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Убираем лишние пробелы\n",
    "    text = re.sub(r\"[\\n\\r]+\", \" \", text)  # Заменяем переносы строк на пробелы\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_rubrics(rubrics_: str) -> str:\n",
    "    return rubrics_.lower().strip()\n",
    "\n",
    "\n",
    "def clean_name_ru(name_ru: str) -> str:\n",
    "    return name_ru.lower().strip()\n",
    "\n",
    "\n",
    "def clean_address(address: str) -> str:\n",
    "    address = address.strip()\n",
    "    return re.sub(r\"\\s+\", \" \", address)\n",
    "\n",
    "\n",
    "work_data['text'] = work_data['text'].apply(clean_review_text)\n",
    "work_data['rubrics'] = work_data['rubrics'].apply(clean_rubrics)\n",
    "work_data['name_ru'] = work_data['name_ru'].apply(clean_name_ru)\n",
    "work_data['address'] = work_data['address'].apply(clean_address)\n",
    "\n",
    "# Преобразование рейтинга в числовой формат\n",
    "work_data['rating'] = work_data['rating'].astype(float)\n",
    "\n",
    "# Удаление строк с пропущенными значениями\n",
    "work_data.dropna(subset=['address', 'name_ru', 'rubrics', 'rating', 'text'], inplace=True)\n",
    "work_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ACDhH98073Ht",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "work_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33Za3O67HwSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Убедитесь, что у вас загружены стоп-слова\n",
    "nltk.download('stopwords', quiet=True)\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# Функция для извлечения трех ключевых слов\n",
    "def extract_keywords(text):\n",
    "    text = text.strip()\n",
    "    if not text:  # Проверка на пустую строку\n",
    "        return ''\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=russian_stopwords, max_features=3)\n",
    "\n",
    "    try:\n",
    "        X = vectorizer.fit_transform([text])\n",
    "        # Извлекаем ключевые слова\n",
    "        keywords = vectorizer.get_feature_names_out()\n",
    "        return ', '.join(keywords)\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx19FGdWHwUk"
   },
   "outputs": [],
   "source": [
    "work_data['key_words'] = work_data['text'].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is0GxZS6RdIm"
   },
   "outputs": [],
   "source": [
    "work_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAQPHF8oRhgT"
   },
   "outputs": [],
   "source": [
    "weird_data = work_data[work_data['key_words']=='']\n",
    "weird_data.info()\n",
    "#проще удалить эти отзывы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DR8K_DMnR33a"
   },
   "outputs": [],
   "source": [
    "work_data = work_data[work_data['key_words']!='']\n",
    "work_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO9FRF0FctHP"
   },
   "outputs": [],
   "source": [
    "work_data.to_csv('work_data.csv', index=False)\n",
    "\n",
    "print(\"Данные успешно очищены и сохранены в 'work_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c0be9e-8fc2-4fa4-911c-8f73b537e14c",
     "showTitle": false,
     "title": ""
    },
    "id": "7lIRmY2fVhGH"
   },
   "source": [
    "#LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q27OdWHss3vW"
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate --progress-bar off\n",
    "!pip install -q peft --progress-bar off\n",
    "!pip install -q bitsandbytes --progress-bar off\n",
    "!pip install -q transformers --progress-bar off\n",
    "!pip install -q trl --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXgqdVLws3sr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsiasFvys3qI"
   },
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
    "\n",
    "    :param load_in_4bit: Load model in 4-bit precision mode\n",
    "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
    "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
    "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzRvLZRNs3nk"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def load_model(model_name, bnb_config):\n",
    "    \"\"\"\n",
    "    Loads model and model tokenizer\n",
    "\n",
    "    :param model_name: Hugging Face model name\n",
    "    :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "    time.sleep(1)\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Load model tokenizer with the user authentication token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = False)\n",
    "\n",
    "    # Set padding token as EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puItQD47tlfl"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################################################################\n",
    "# transformers parameters\n",
    "################################################################################\n",
    "\n",
    "# The pre-trained model from the Hugging Face Hub to load and fine-tune\n",
    "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "#model_name = \"daryl149/llama-2-7b-chat-hf\"\n",
    "#model_name = \"OpenBuddy/openbuddy-llama2-13b-v8.1-fp16\"\n",
    "#model_name = \"OpenBuddy/openbuddy-llama-7b-v4-fp16\"\n",
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "#model_name = \"rccmsu/ruadapt_llama2_7b_v0.1\"\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml1I96uMvaKS"
   },
   "outputs": [],
   "source": [
    "# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n",
    "\n",
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut9H6uP0s3lL"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQsdgLh44yM-"
   },
   "source": [
    "### Загрузка датасета\n",
    "\n",
    "Будем использовать заранее подготовленный датасет с инструкциями.\n",
    "\n",
    "В данном случае это csv-таблица с 3 колонками:\n",
    "\n",
    "* Инструкция\n",
    "* Текст\n",
    "* Класс\n",
    "\n",
    "Мы будем использовать стандартный генератор датасета типа `csv` (потому что у нас файл CSV). Аналогично здесь мог бы быть файл JSON и типа датасет `json`. По умолчанию все записи относятся к разделению `train`, который мы получим с помощью параметра `split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbcKeb4o8fAa"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files = '/content/work_data.csv', split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLoaZvdTmIZT"
   },
   "outputs": [],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6hxLRYZ7Fwd"
   },
   "source": [
    "Функция `load_dataset` преобразует файл CSV в словарь промтов. Мы можем просмотреть объекты, используя случайный индекс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulIcDOQVpun3"
   },
   "outputs": [],
   "source": [
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmOzs7Wj6NnR"
   },
   "source": [
    "### Создание шаблона промта\n",
    "\n",
    "Определим функцию `create_prompt_formats` для создания промта для модели на основе содержания промтов в датасте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgLM5FR0mSP2"
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    categories = sample['rubrics'].split(';')  # Предполагается, что рубрики разделены запятыми\n",
    "    categories = [cat.strip() for cat in categories]\n",
    "    categories_str = ', '.join(categories)\n",
    "    categories = f\"Категории:\\n{categories_str}\"\n",
    "\n",
    "    rating = f\"Рейтинг:\\n{sample['rating']}\"\n",
    "    keywords = f\"Ключевые слова:\\n{sample['key_words']}\"\n",
    "    rewiev = f\"Отзыв:\\n{sample['text']}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [categories, rating, keywords, rewiev] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"prompt\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iL1qmnMER4nH"
   },
   "outputs": [],
   "source": [
    "create_prompt_formats(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91ctsLt9G2Gc"
   },
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9M1e7HpDh2N"
   },
   "outputs": [],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V42Lk6Vx6y6P"
   },
   "source": [
    "### Получение максимальной длины последовательности предобученной модели\n",
    "\n",
    "Определим функцию `get_max_length` для определения максимальной длины посделовательности для Llama-2-7B model.\n",
    "\n",
    "Эта функция извлечет конфигурацию модели и попытается найти максимальную длину последовательности из одного из нескольких ключей конфигурации, которые могут ее содержать. Если максимальная длина последовательности не найдена, по умолчанию она будет равна 1024. Мы будем использовать максимальную длину последовательности во время предварительной обработки датасета, чтобы удалить записи, которые превышают эту длину контекста, поскольку предварительно обученная модель не примет их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6bAo-8nmtFn"
   },
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNs971cLTTTZ"
   },
   "source": [
    "### Токенизирование батчей\n",
    "\n",
    "Функция `preprocess_batch` будет токенизировать входящий (`batch`) используя  `tokenizer`. Мы устанавливаем параметр максимальной длины последовательности  `max_length`, от которго будет зависеть паддинг либо обрезка данных.\n",
    "\n",
    "Параметр `truncation = True` приведёт к обрезанию длинных текстов.\n",
    "\n",
    "Параметр `padding = max_length` будет производить паддинг до максимальной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7vwFmJhmt6t"
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"prompt\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rsrRaiMDdIb"
   },
   "source": [
    "### Предобработка датасета\n",
    "\n",
    "1. Создание промтов через функцию `create_prompt_formats`.\n",
    "2. Токенизирование батчей через функцию `preprocess_batch`, удаление исходных колонок (instruction, input, output, text).\n",
    "3. Фильтрация итоговых промтов по максимальной длине в токенах.\n",
    "4. Перемешивание датасета (shuffle) с инициализацией random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9UkOnqgmvlZ"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = ['address', 'name_ru', 'rating', 'rubrics', 'text','key_words'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50XY9r6ssu-x"
   },
   "outputs": [],
   "source": [
    "# Random seed\n",
    "seed = 11111\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jepKTqUuIgID"
   },
   "source": [
    "Вот так выглядит теперь датасет, состоящий из токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-aACsuus0ps"
   },
   "outputs": [],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wN2y7yAIfN1"
   },
   "outputs": [],
   "source": [
    "print(preprocessed_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOYguJcERvOY"
   },
   "source": [
    "With everything set up, we can move forward to fine-tuning or instruction-tuning Llama-2-7B on our news classification instruction dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhI55wj7R9gd"
   },
   "source": [
    "### Создание конфигурации PEFT\n",
    "\n",
    "Подход PEFT позволяет тюнить небольшое количество дополнительных параметров модели, одновременно замораживая большинство параметров предварительно обученных LLM, значительно снижая затраты на вычисления и хранение. Это также помогает обеспечить переносимость: пользователи могут настраивать модели с помощью методов PEFT, чтобы получить LoRa-модули в размером в несколько МБ.\n",
    "\n",
    "\n",
    "Воспользуемся библиотекой `peft` из Hugging Face.\n",
    "\n",
    "Существует несколько методов PEFT. Мы будем использовать QLoRA, применяя класс `LoraConfig` из библиотеки  `peft`.\n",
    "\n",
    "QLoRA квантует модель в 4 бита, затем замораживает веса основной модели и добавляет две матрицы обучаемых мараметров. Во время тюнигна, QLoRA пробрасывает градиент через замороженную часть общей модели.\n",
    "\n",
    "Обновляются только веса LoRa-модуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "913uHanlnYef"
   },
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "    \"\"\"\n",
    "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
    "\n",
    "    :param r: LoRA attention dimension\n",
    "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
    "    :param modules: Names of the modules to apply LoRA to\n",
    "    :param lora_dropout: Dropout Probability for LoRA layers\n",
    "    :param bias: Specifies if the bias parameters should be trained\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVKxwcjPbBTq"
   },
   "source": [
    "### Поиск модулей для LoRA\n",
    "\n",
    "Функция `find_all_linear_names` предназначен для поиска слоёв оригинальной сети, для которых будет применяться LoRa.\n",
    "\n",
    "\n",
    "Функция получает названия слоёв через `model.named_modules()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dg8pIUgMm_bX"
   },
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find modules to apply LoRA to.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaaQtyB5b_Ag"
   },
   "source": [
    "### Подсчёт обучаемых параметров\n",
    "\n",
    "Функция `print_trainable_parameters` предназначена для расчёта количества обучаемых параметров в `model.named_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6iEs6pVnCac"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOICPBjig9ri"
   },
   "source": [
    "### Fine-tuning предобученной модели\n",
    "\n",
    "Функция `fine_tune` оборачивает описанные выше модули и запускает их:\n",
    "\n",
    "\n",
    "1. Разрешить сохранение градиентов (gradient checkpointing) для уменьшения использования памяти во время тюнинга.\n",
    "2. Использование функции `prepare_model_for_kbit_training` из PEFT для подготови модели к тюнингу.\n",
    "3. Вызов `find_all_linear_names` для получения названий слоёв сети для применения LoRA .\n",
    "4. Создание конфигурации LoRA через вызов функции `create_peft_config`.\n",
    "5. Оборачивание базовой модели с Hugging Face model для тюнинга через PEFT путём вызова функции `get_peft_model`.\n",
    "6. Печать обучаемых параметров.\n",
    "\n",
    "\n",
    "Для обучения мы инициализируем объект `Trainer()` внутри функции `fine_tune`, который требует:\n",
    "\n",
    "\n",
    "`per_device_train_batch_size`: ,размер батча на обучении.\n",
    "\n",
    "\n",
    "`gradient_accumulation_steps`: количество шагов, для которых необходимо накопить градиенты перед выполнением обратного прохода.\n",
    "\n",
    "\n",
    "`warmup_steps`: количество шагов линейного увеличения скорости обучения от 0 до `learning_rate`.\n",
    "\n",
    "\n",
    "`max_steps`: количество шагов обучения.\n",
    "\n",
    "\n",
    "`learning_rate`: начальный  learning rate для Adam.\n",
    "\n",
    "\n",
    "`fp16`: использовать ли 16-bit (mixed) обучение вместо  32-bit.\n",
    "\n",
    "\n",
    "`logging_steps`: количество шагов между двумя логированиями.\n",
    "\n",
    "\n",
    "`output_dir`: папка для сохранения логов и модели.\n",
    "\n",
    "\n",
    "`optim`: оптимизатор для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCbnhnxtnhvh"
   },
   "outputs": [],
   "source": [
    "def fine_tune(model,\n",
    "          tokenizer,\n",
    "          dataset,\n",
    "          lora_r,\n",
    "          lora_alpha,\n",
    "          lora_dropout,\n",
    "          bias,\n",
    "          task_type,\n",
    "          per_device_train_batch_size,\n",
    "          gradient_accumulation_steps,\n",
    "          warmup_steps,\n",
    "          max_steps,\n",
    "          learning_rate,\n",
    "          fp16,\n",
    "          logging_steps,\n",
    "          output_dir,\n",
    "          optim):\n",
    "    \"\"\"\n",
    "    Prepares and fine-tune the pre-trained model.\n",
    "\n",
    "    :param model: Pre-trained Hugging Face model\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param dataset: Preprocessed training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare the model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get LoRA module names\n",
    "    target_modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        train_dataset = dataset,\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_steps = warmup_steps,\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = fp16,\n",
    "            logging_steps = logging_steps,\n",
    "            output_dir = output_dir,\n",
    "            optim = optim,\n",
    "            report_to=\"tensorboard\",####/\n",
    "\n",
    "        ),\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training and log metrics\n",
    "    print(\"Training...\")\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP3k_qjEvvSF"
   },
   "source": [
    "Используем парамерты QLoRa для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxC2aY8eUzH7"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 64\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Bias\n",
    "bias = \"none\"\n",
    "\n",
    "# Task type\n",
    "task_type = \"CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnfOKEvipXLG"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 20 #0\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRJZ6L7qpPIM"
   },
   "outputs": [],
   "source": [
    "fine_tune(model,\n",
    "      tokenizer,\n",
    "      preprocessed_dataset,\n",
    "      lora_r,\n",
    "      lora_alpha,\n",
    "      lora_dropout,\n",
    "      bias,\n",
    "      task_type,\n",
    "      per_device_train_batch_size,\n",
    "      gradient_accumulation_steps,\n",
    "      warmup_steps,\n",
    "      max_steps,\n",
    "      learning_rate,\n",
    "      fp16,\n",
    "      logging_steps,\n",
    "      output_dir,\n",
    "      optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxP2DWAx-5xa"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "def generate_review(category, rating, keywords, max_length=150, temperature=0.7, top_k=50, top_p=0.95):\n",
    "    prompt = f\"Категория: {category}\\nРейтинг: {rating}\\nКлючевые слова: {keywords}\\nОтзыв:\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Извлечение только текста отзыва\n",
    "    review = generated_text.split('Отзыв:')[-1].strip()\n",
    "    return review\n",
    "\n",
    "\n",
    "category = \"ресторан\"\n",
    "rating = 5\n",
    "keywords = \"паста\"\n",
    "\n",
    "generated_review = generate_review(category, rating, keywords, 15, 1, 100000, 1.5)\n",
    "print(\"Сгенерированный отзыв:\")\n",
    "print(generated_review)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02L - LoRA with PEFT",
   "widgets": {}
  },
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
